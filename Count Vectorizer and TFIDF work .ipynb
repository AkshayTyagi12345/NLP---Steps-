{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c48b42ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "# test classification ---\n",
    "from sys import argv\n",
    "import string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afead321",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence 1: India won the match \n",
      "Sentence 2: England won the cricket match \n",
      "Sentence 3: Australia won the final match\n"
     ]
    }
   ],
   "source": [
    "#Method 2 -- \n",
    "#Inputting string from user \n",
    "sent1=input(\"Sentence 1: \")\n",
    "sent2=input(\"Sentence 2: \")\n",
    "sent3=input(\"Sentence 3: \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6840ccb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "type(sent1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "891b1404",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''Write a program to input three sentences from user and creates the corpus\n",
    "\n",
    "Create a function named “MakeCorpus” which will take list of string as an input and will return a list having union of all words.\n",
    "Save this function in a python file named “Corpus”. This can be used for future applications\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d08cb4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14318d93",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58323a05",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5f3829b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def MakeCorpus(list_of_strings):\n",
    "    allWords=[]\n",
    "    for each in list_of_strings:\n",
    "        words=nltk.word_tokenize(each)\n",
    "        for every in words:\n",
    "            allWords.append(every)\n",
    "    corpus=list(set(allWords))\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b00778c6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Australia', 'England', 'cricket', 'final', 'India', 'match', 'the', 'won']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list_of_inputs=[sent1,sent2,sent3]\n",
    "MakeCorpus(list_of_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6523ce8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['India won the match ', 'England won the cricket match ', 'Australia won the final match']\n"
     ]
    }
   ],
   "source": [
    "print(list_of_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9cf9c477",
   "metadata": {},
   "outputs": [],
   "source": [
    "Write a program to input three sentences from user and convert them into vectors.  \n",
    "Use presence and absence of words to build the vectors.\n",
    "\n",
    "Create a function named “PresenceAbsenceVectorization” which will take list of string as an input and will return a list of vectors.\n",
    "Save this function in a python file named “Vectorization”. This can be used for future applications"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "88f5c43c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def PresenceAbsenceVectorization(list_of_string):\n",
    "    corpus=MakeCorpus(list_of_string)\n",
    "    list_of_vectors=[]\n",
    "    for each in list_of_string:\n",
    "        words=nltk.word_tokenize(each)\n",
    "        vector=[]\n",
    "        for every in corpus:\n",
    "            if every in words:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        list_of_vectors.append(vector)\n",
    "    return list_of_vectors\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "0e9d9000",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[0, 0, 0, 0, 1, 1, 1, 1], [0, 1, 1, 0, 0, 1, 1, 1], [1, 0, 0, 1, 0, 1, 1, 1]]"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PresenceAbsenceVectorization(list_of_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "20ebafb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CountVectorization(list_of_string):\n",
    "    corpus=MakeCorpus(list_of_string)\n",
    "    list_of_vectors=[]\n",
    "    for each in list_of_string:\n",
    "        words=nltk.word_tokenize(each)\n",
    "        vector=[]\n",
    "        for every in corpus:\n",
    "            if every in words:\n",
    "                vector.append(words.count(every))\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        list_of_vectors.append(vector)\n",
    "    return list_of_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "81eeb04c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[1, 1, 1, 2, 1, 0, 1, 2], [1, 1, 1, 0, 1, 1, 2, 2], [1, 2, 1, 1, 1, 1, 0, 2]]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "CountVectorization([\"A lives with B. A plays with C\",\"B lives with C . B plays with D\",\"C lives with D . C plays with A\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "f1e4963a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "672b9d0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def TFIDFVectorization(list_of_string):\n",
    "    vectorizer=TfidfVectorizer()\n",
    "    tf_idfMatrix=vectorizer.fit_transform(list_of_string).toarray()\n",
    "    return list(tf_idfMatrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1c6f28e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[array([0.        , 0.        , 0.        , 0.        , 0.69903033,\n",
       "        0.41285857, 0.41285857, 0.41285857]),\n",
       " array([0.        , 0.57292883, 0.57292883, 0.        , 0.        ,\n",
       "        0.338381  , 0.338381  , 0.338381  ]),\n",
       " array([0.57292883, 0.        , 0.        , 0.57292883, 0.        ,\n",
       "        0.338381  , 0.338381  , 0.338381  ])]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "TFIDFVectorization(list_of_inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763fb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "# VACTORIZATION FILE "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ad5d3e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "def MakeCorpus(list_of_strings):\n",
    "    allWords=[]\n",
    "    for each in list_of_strings:\n",
    "        words=nltk.word_tokenize(each)\n",
    "        for every in words:\n",
    "            allWords.append(every)\n",
    "    corpus=list(set(allWords))\n",
    "    return corpus\n",
    "\n",
    "def PresenceAbsenceVectorization(list_of_string):\n",
    "    corpus=MakeCorpus(list_of_string)\n",
    "    list_of_vectors=[]\n",
    "    for each in list_of_string:\n",
    "        words=nltk.word_tokenize(each)\n",
    "        vector=[]\n",
    "        for every in corpus:\n",
    "            if every in words:\n",
    "                vector.append(1)\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        list_of_vectors.append(vector)\n",
    "    return list_of_vectors\n",
    "\n",
    "def CountVectorization(list_of_string):\n",
    "    corpus=MakeCorpus(list_of_string)\n",
    "    list_of_vectors=[]\n",
    "    for each in list_of_string:\n",
    "        words=nltk.word_tokenize(each)\n",
    "        vector=[]\n",
    "        for every in corpus:\n",
    "            if every in words:\n",
    "                vector.append(words.count(every))\n",
    "            else:\n",
    "                vector.append(0)\n",
    "        list_of_vectors.append(vector)\n",
    "    return list_of_vectors\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def TFIDFVectorization(list_of_string):\n",
    "    vectorizer=TfidfVectorizer()\n",
    "    tf_idfMatrix=vectorizer.fit_transform(list_of_string).toarray()\n",
    "    return list(tf_idfMatrix)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0af1448",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be1efa48",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfc0a692",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8a90fa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7ca2a95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402a71c7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bab0914",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
